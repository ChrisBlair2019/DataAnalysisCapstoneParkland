{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n[nltk_data]     C:\\Users\\Chris\\AppData\\Roaming\\nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import indicoio, json, re\n",
    "from urlextract import URLExtract\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline \n",
    "import datetime\n",
    "from empath import Empath\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "from gensim import corpora\n",
    "from gensim.models import LsiModel\n",
    "from gensim.models import LdaModel\n",
    "import nltk\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "import plotly.plotly.plotly\n",
    "from plotly.offline import init_notebook_mode, plot, iplot\n",
    "import plotly.graph_objs as go\n",
    "import plotly.io as pio\n",
    "import plotly.plotly as py\n",
    "import plotly.graph_objs as go\n",
    "analyser = SentimentIntensityAnalyzer()\n",
    "lexicon = Empath()\n",
    "nltk.download('stopwords')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "indicoio.config.api_key = '797527d8dbe8cbb44e9a4019cd690afd'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of comments in our dataset\n50500\n"
     ]
    }
   ],
   "source": [
    "with open('result.json', 'r') as myfile:\n",
    "    data = json.loads(myfile.read())\n",
    "\n",
    "\n",
    "count = 0\n",
    "    \n",
    "for x in data['comments']:\n",
    "    count += 1\n",
    "\n",
    "print(\"Number of comments in our dataset\")\n",
    "print(count)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "extractor = URLExtract()\n",
    "\n",
    "timestamp = []\n",
    "sentiment = []\n",
    "political = []\n",
    "\n",
    "\n",
    "\n",
    "num = 0\n",
    "for x in data['comments']:\n",
    "    num += 1\n",
    "    \n",
    "    # TEXT CLEANING\n",
    "    input_str = data['comments'][x]['body'].lower() # all letters become lowercase \n",
    "    \n",
    "    url = extractor.find_urls(input_str)          # extracts and removes url\n",
    "    if(len(url) != 0):\n",
    "        input_str = input_str.replace(url[0],'')\n",
    "\n",
    "    input_str = re.sub(r'\\d+', '', input_str)     # removes numbers\n",
    "    input_str = input_str.strip()                 # removes whitespace\n",
    "    input_str = re.sub(r'[^\\w\\s]', '', input_str)   # removes punctuation\n",
    "    \n",
    "    \n",
    "    \n",
    "    # STORING THE TIMESTAMP, SENTIMENT, POLITICAL DATA IN SEPARATE LISTS\n",
    "    if( 'timestamp' in data['comments'][x] and input_str):\n",
    "        timestamp.append(data['comments'][x]['timestamp'])\n",
    "        #sentiment.append(indicoio.sentiment(input_str))\n",
    "        #political.append(indicoio.political(input_str))\n",
    "        \n",
    "    data['comments'][x]['body'] = input_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, '0.633*\"gun\" + 0.270*\"peopl\" + 0.157*\"would\" + 0.154*\"like\" + 0.153*\"dont\" + 0.132*\"right\" + 0.124*\"get\" + 0.119*\"one\" + 0.111*\"law\" + 0.107*\"think\" + 0.103*\"shoot\" + 0.101*\"make\" + 0.099*\"use\" + 0.098*\"go\" + 0.097*\"want\"'), (1, '-0.704*\"gun\" + 0.307*\"peopl\" + 0.178*\"like\" + 0.167*\"right\" + 0.160*\"dont\" + 0.121*\"would\" + 0.109*\"think\" + 0.107*\"say\" + 0.104*\"get\" + 0.102*\"im\" + 0.095*\"one\" + 0.091*\"school\" + 0.085*\"go\" + 0.083*\"thing\" + 0.076*\"make\"'), (2, '0.654*\"peopl\" + -0.470*\"right\" + -0.180*\"amend\" + 0.174*\"shoot\" + 0.163*\"school\" + 0.153*\"kill\" + -0.144*\"would\" + -0.137*\"law\" + -0.126*\"state\" + -0.084*\"constitut\" + -0.083*\"govern\" + 0.080*\"mass\" + -0.079*\"us\" + -0.072*\"second\" + -0.067*\"arm\"'), (3, '-0.513*\"peopl\" + 0.479*\"school\" + -0.361*\"right\" + 0.275*\"shoot\" + 0.137*\"one\" + 0.124*\"student\" + 0.113*\"fire\" + -0.102*\"amend\" + 0.101*\"high\" + 0.093*\"shot\" + 0.092*\"polic\" + 0.090*\"mass\" + 0.086*\"use\" + -0.086*\"gun\" + 0.084*\"year\"'), (4, '0.394*\"right\" + -0.276*\"dont\" + 0.270*\"school\" + 0.222*\"firearm\" + 0.221*\"peopl\" + -0.205*\"like\" + 0.181*\"state\" + -0.158*\"im\" + -0.157*\"think\" + 0.153*\"use\" + -0.140*\"get\" + 0.136*\"law\" + -0.126*\"want\" + 0.116*\"shoot\" + 0.110*\"amend\"'), (5, '-0.413*\"would\" + 0.377*\"school\" + 0.370*\"right\" + -0.254*\"us\" + -0.214*\"firearm\" + -0.184*\"use\" + 0.138*\"gun\" + -0.136*\"suicid\" + -0.136*\"homicid\" + -0.124*\"rate\" + 0.122*\"dont\" + 0.111*\"student\" + -0.106*\"state\" + -0.104*\"govern\" + -0.102*\"law\"')]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "# create English stop words list\n",
    "en_stop = set(stopwords.words('english'))\n",
    "# Create p_stemmer of class PorterStemmer\n",
    "p_stemmer = PorterStemmer()\n",
    "# list for tokenized documents in loop\n",
    "texts = []\n",
    "# loop through document list\n",
    "for i in data['comments']:\n",
    "    # clean and tokenize document string\n",
    "    raw = data['comments'][i]['body'].lower()\n",
    "    tokens = tokenizer.tokenize(raw)\n",
    "    # remove stop words from tokens\n",
    "    stopped_tokens = [i for i in tokens if not i in en_stop]\n",
    "    # stem tokens\n",
    "    stemmed_tokens = [p_stemmer.stem(i) for i in stopped_tokens]\n",
    "    # add tokens to list\n",
    "    texts.append(stemmed_tokens)\n",
    "\n",
    "\n",
    "dictionary = corpora.Dictionary(texts)\n",
    "# Converting list of documents (corpus) into Document \n",
    "# Term Matrix using dictionary prepared above.\n",
    "doc_term_matrix = [dictionary.doc2bow(doc) for doc in texts]\n",
    "number_of_topics = 6\n",
    "# generate LSA model\n",
    "lsamodel = LsiModel(doc_term_matrix, num_topics=number_of_topics, id2word = dictionary)  # train model\n",
    "print(lsamodel.print_topics(num_topics=number_of_topics, num_words=15))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-0894b42d68c0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[0mstopped_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtokens\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0men_stop\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[0;31m# stem tokens\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m     \u001b[0mstemmed_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mp_stemmer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstopped_tokens\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m     \u001b[0;31m# add tokens to list\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0mtexts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstemmed_tokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-0894b42d68c0>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[0mstopped_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtokens\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0men_stop\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[0;31m# stem tokens\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m     \u001b[0mstemmed_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mp_stemmer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstopped_tokens\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m     \u001b[0;31m# add tokens to list\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0mtexts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstemmed_tokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\Chris\\AppData\\Local\\Programs\\Python\\Python37-32\\lib\\site-packages\\nltk\\stem\\porter.py\u001b[0m in \u001b[0;36mstem\u001b[0;34m(self, word)\u001b[0m\n\u001b[1;32m    668\u001b[0m         \u001b[0mstem\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_step1b\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    669\u001b[0m         \u001b[0mstem\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_step1c\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 670\u001b[0;31m         \u001b[0mstem\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_step2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    671\u001b[0m         \u001b[0mstem\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_step3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    672\u001b[0m         \u001b[0mstem\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_step4\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\Chris\\AppData\\Local\\Programs\\Python\\Python37-32\\lib\\site-packages\\nltk\\stem\\porter.py\u001b[0m in \u001b[0;36m_step2\u001b[0;34m(self, word)\u001b[0m\n\u001b[1;32m    512\u001b[0m             \u001b[0mrules\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"logi\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"log\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_has_positive_measure\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    513\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 514\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply_rule_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrules\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    515\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    516\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_step3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\Chris\\AppData\\Local\\Programs\\Python\\Python37-32\\lib\\site-packages\\nltk\\stem\\porter.py\u001b[0m in \u001b[0;36m_apply_rule_list\u001b[0;34m(self, word, rules)\u001b[0m\n\u001b[1;32m    265\u001b[0m                     \u001b[0;31m# Don't try any further rules\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    266\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 267\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msuffix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    268\u001b[0m                 \u001b[0mstem\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_replace_suffix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msuffix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    269\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mcondition\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mcondition\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "with open('result.json', 'r') as myfile:\n",
    "    data2 = json.loads(myfile.read())\n",
    "    \n",
    "extractor = URLExtract()\n",
    "\n",
    "timestamp = []\n",
    "sentiment = []\n",
    "political = []\n",
    "\n",
    "num = 0\n",
    "for x in data2['comments']:\n",
    "    num += 1\n",
    "    \n",
    "    # TEXT CLEANING\n",
    "    input_str = data2['comments'][x]['body'].lower() # all letters become lowercase \n",
    "    \n",
    "    url = extractor.find_urls(input_str)          # extracts and removes url\n",
    "    if(len(url) != 0):\n",
    "        input_str = input_str.replace(url[0],'')\n",
    "\n",
    "    input_str = re.sub(r'\\d+', '', input_str)     # removes numbers\n",
    "    input_str = input_str.strip()                 # removes whitespace\n",
    "    input_str = re.sub(r'[^\\w\\s]', '', input_str)   # removes punctuation\n",
    "    \n",
    "    \n",
    "    \n",
    "    # STORING THE TIMESTAMP, SENTIMENT, POLITICAL DATA IN SEPARATE LISTS\n",
    "    if( 'timestamp' in data2['comments'][x] and input_str):\n",
    "        timestamp.append(data2['comments'][x]['timestamp'])\n",
    "        #sentiment.append(indicoio.sentiment(input_str))\n",
    "        #political.append(indicoio.political(input_str))\n",
    "        \n",
    "    data2['comments'][x]['body'] = input_str\n",
    "    \n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "# create English stop words list\n",
    "en_stop = set(stopwords.words('english'))\n",
    "# Create p_stemmer of class PorterStemmer\n",
    "p_stemmer = PorterStemmer()\n",
    "# list for tokenized documents in loop\n",
    "texts = []\n",
    "# loop through document list\n",
    "for i in data2['comments']:\n",
    "    # clean and tokenize document string\n",
    "    raw = data2['comments'][i]['body'].lower()\n",
    "    tokens = tokenizer.tokenize(raw)\n",
    "    # remove stop words from tokens\n",
    "    stopped_tokens = [i for i in tokens if not i in en_stop]\n",
    "    # stem tokens\n",
    "    stemmed_tokens = [p_stemmer.stem(i) for i in stopped_tokens]\n",
    "    # add tokens to list\n",
    "    texts.append(stemmed_tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['soul', 'heart', 'beauty', 'thou', 'sea', 'outward', 'graceful', 'mother', 'souls', 'human']\n['america', 'hardly', 'extent', 'democratic', 'states', 'population', 'parties', 'falls', 'americans', 'interests']\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"C:\\Git Folder\\SAGE\\py-sage\")\n",
    "import sage\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "def getCountDict(filename):\n",
    "    with open(filename) as fin:\n",
    "        return {word:int(count) for word,count in [line.rstrip().split() for line in fin.readlines()]}\n",
    "    \n",
    "# counts for author Lydia Maria Child\n",
    "child_counts = getCountDict('underwood-child-counts.tsv')\n",
    "# counts for all 1840s letters in the corpus \n",
    "base_counts = getCountDict('underwood-1840s-let-counts.tsv')\n",
    "\n",
    "vocab = [word for word,count in Counter(child_counts).most_common(5000)]\n",
    "\n",
    "x_child = np.array([child_counts[word] for word in vocab])\n",
    "x_base = np.array([base_counts[word] for word in vocab]) + 1.\n",
    "\n",
    "mu = np.log(x_base) - np.log(x_base.sum())\n",
    "\n",
    "eta = sage.estimate(x_child,mu)\n",
    "\n",
    "print(sage.topK(eta,vocab))\n",
    "\n",
    "print(sage.topK(-eta,vocab))\n",
    "\n",
    "plt.hist(eta,20);\n",
    "\n",
    "plt.plot(sorted(eta));\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
