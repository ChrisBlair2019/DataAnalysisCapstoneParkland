{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import nltk\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem import PorterStemmer\n",
    "from urlextract import URLExtract\n",
    "stemmer = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "extractor = URLExtract()\n",
    "\n",
    "def clean_text(text, lemmatize=False, stem=False):\n",
    "    # lower case\n",
    "    text = text.lower()\n",
    "    \n",
    "    # removes URLs\n",
    "    urls = set(extractor.find_urls(text))\n",
    "    text = ' '.join([t for t in text.split(' ') if t not in urls])\n",
    "    \n",
    "    # remove numbers\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    \n",
    "    # remove punctuations\n",
    "    for p in \"=$“”%.,!?:;\\\"'_-~|&[]#*()’<>/\\\\\":\n",
    "        text = text.replace(p,' ')\n",
    "    \n",
    "    # remove '\\n', '\\t'\n",
    "    text = text.replace('\\n', '')\n",
    "    text = text.replace('\\t', '')\n",
    "    text = ' '.join(text.split())\n",
    "    text_tokens = word_tokenize(text)\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    \n",
    "    tokens = [i for i in text_tokens if i not in stop_words]\n",
    "    \n",
    "    # lemmatize tokens\n",
    "    if lemmatize:\n",
    "        tokens = [lemmatizer.lemmatize(i) for i in tokens]\n",
    "        \n",
    "    # stemming tokens\n",
    "    if stem:\n",
    "        tokens = [stemmer.stem(i) for i in tokens]\n",
    "    \n",
    "    tokens = [i for i in tokens if i not in stop_words]\n",
    "    return text, tokens\n",
    "\n",
    "def pland_clean(tokens):\n",
    "    custom = [\"www\", \"http\", \"le\", \"https\", \"com\", \"said\", \"would\", \"people\", \"u\", \"like\", \"r\"]\n",
    "    custom += [\"thing\", \"think\", \"one\", \"know\", \"say\", \"well\", \"deleted\", \"really\", \"reddit\"]\n",
    "    custom += [\"comment\", \"please\", \"yes\", \"going\", \"get\", \"yeah\", \"read\", \"link\", \"also\", \"could\"]\n",
    "    custom += [\"getting\", \"got\", \"ok\", \"lol\", \"exactly\", \"oh\", \"gon\", \"na\", \"want\", \"make\", \"take\", \"removed\"]\n",
    "    custom += [\"someone\", \"anything\", \"someone\", \"im\", \"many\", \"even\", \"much\", \"anyone\", \"way\", \"go\"]\n",
    "    custom += [\"saying\",\"something\",\"anywhere\", \"actually\", \"guy\", \"kid\", \"point\", \"see\", \"country\"]\n",
    "    custom += [\"talking\", \"nothing\", \"year\", \"let\", \"every\", \"any\", \"mean\", \"keep\", \"never\", \"meeting\"]\n",
    "    custom += [\"maybe\", \"news\", \"lot\", \"en\"]\n",
    "    custom = set(custom)\n",
    "    tokens = [i for i in tokens if i not in custom]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loads json\n",
    "with open('./data/parkland_ext.json') as pland_file:\n",
    "    pland = json.load(pland_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# BFS out on a particular post\n",
    "post_replies = list()\n",
    "comment_queue = list()\n",
    "# comment_queue.append('t3_87vnah')\n",
    "for post in pland['posts']:\n",
    "    comment_queue.append(post['fullname'])\n",
    "while comment_queue:\n",
    "    comment_fullname = comment_queue.pop(0)\n",
    "    post_replies.append(comment_fullname)\n",
    "    \n",
    "    for reply_fn in pland['comments'][comment_fullname]['replies']:\n",
    "        comment_queue.append(reply_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "comp_tokens = list()\n",
    "for fn in post_replies:\n",
    "    comp = pland['comments'][fn]['body']\n",
    "    cleaned, tokens = clean_text(comp, lemmatize=True, stem=False)\n",
    "    \n",
    "    # just nouns\n",
    "#     tokens = word_tokenize(cleaned)\n",
    "#     tokens = [k[0] for k in nltk.pos_tag(tokens) if k[1] == 'NN' or k[1] == 'NNS']\n",
    "#     stop_words = set(stopwords.words('english'))\n",
    "#     tokens = [i for i in tokens if i not in stop_words]\n",
    "#     tokens = [lemmatizer.lemmatize(i) for i in tokens]\n",
    "    \n",
    "    tokens = pland_clean(tokens)\n",
    "    comp_tokens.append(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import corpora\n",
    "dictionary = corpora.Dictionary(comp_tokens)\n",
    "corpus = [dictionary.doc2bow(token) for token in comp_tokens]\n",
    "\n",
    "# remove tokens that only appeared once and more than \n",
    "updated_comp_tokens = list()\n",
    "lower = 1\n",
    "upper = 2800\n",
    "min_token_len = 10\n",
    "outliers = set([dictionary[k] for k in dictionary.dfs if dictionary.dfs[k] <= lower or dictionary.dfs[k] >= upper])\n",
    "for tokens in comp_tokens:\n",
    "    if len(tokens) < min_token_len:\n",
    "        continue\n",
    "    tokens = [token for token in tokens if token not in outliers]\n",
    "    updated_comp_tokens.append(tokens)\n",
    "\n",
    "dictionary = corpora.Dictionary(updated_comp_tokens)\n",
    "corpus = [dictionary.doc2bow(token) for token in updated_comp_tokens]\n",
    "\n",
    "import pickle\n",
    "# pickle.dump(corpus, open('corpus.pkl', 'wb'))\n",
    "# dictionary.save('dictionary.gensim')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25916"
      ]
     },
     "execution_count": 313,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(updated_comp_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Latent Dirchlet Allocation\n",
    "NUM_TOPICS = 8\n",
    "ldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics = NUM_TOPICS, id2word=dictionary, passes=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, '0.019*\"amendment\" + 0.011*\"militia\" + 0.008*\"constitution\" + 0.008*\"second\" + 0.008*\"regulated\" + 0.006*\"first\"')\n",
      "(1, '0.014*\"police\" + 0.010*\"shooter\" + 0.007*\"cop\" + 0.006*\"job\" + 0.005*\"armed\" + 0.005*\"shot\"')\n",
      "(2, '0.031*\"nra\" + 0.018*\"source\" + 0.010*\"wiki\" + 0.008*\"org\" + 0.008*\"member\" + 0.007*\"rule\"')\n",
      "(3, '0.013*\"firearm\" + 0.010*\"check\" + 0.010*\"state\" + 0.008*\"background\" + 0.007*\"weapon\" + 0.007*\"violence\"')\n",
      "(4, '0.010*\"control\" + 0.010*\"owner\" + 0.009*\"firearm\" + 0.008*\"crime\" + 0.008*\"issue\" + 0.008*\"buy\"')\n",
      "(5, '0.013*\"trump\" + 0.009*\"party\" + 0.008*\"republican\" + 0.008*\"vote\" + 0.007*\"shit\" + 0.006*\"democrat\"')\n",
      "(6, '0.010*\"mental\" + 0.010*\"crime\" + 0.009*\"rate\" + 0.008*\"court\" + 0.008*\"health\" + 0.008*\"violence\"')\n",
      "(7, '0.013*\"weapon\" + 0.011*\"government\" + 0.011*\"rifle\" + 0.008*\"assault\" + 0.008*\"ban\" + 0.007*\"ar\"')\n"
     ]
    }
   ],
   "source": [
    "topics = ldamodel.print_topics(num_words=6)\n",
    "for topic in topics:\n",
    "    print(topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ldamodel.save('model8_0.gensim')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([(2, 0.56327313),\n",
       "             (0, 0.13315608),\n",
       "             (4, 0.112918414),\n",
       "             (1, 0.08654252),\n",
       "             (3, 0.070896775),\n",
       "             (5, 0.0332131)])"
      ]
     },
     "execution_count": 287,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_doc = \"  doesnt make them a cult fairly certain i said they werent a cult  fairly certain  mental impairments included people who needed help balancing a check book or paying their bills online  that isnt enough justification to deprive people of their constitutional rights  they are recieving government assistance through social security because they have hard enough mental impairments that they cant do something like balance a checkbook or pay their bills online  these are menial tasks that you would expect any adult to be able to do so if you need government assistance to do this again maybe you shouldnt own a gun  the cdc was politicized in the s which lead to a ban on research with the goal of pushing gun control they werent politicized its just the research they were doing lead to a presumption that guns should be more regulated especially when it comes to people with mental illnesses operating them  you cant spin stopping people with mental illnesses from owning and operating guns as a bad thing  look at newtown you want that to happen again lanza had severe mental issues and while he himself didnt own guns his mother did  he had ready access to them   if the nra is standing up for rights and you stand against the nra then what does that make youdo you know what the no true scotsman argument is its when you appeal to purity or in this case constitutional authority as a way to dismiss valid criticism  the nra stands against reforming the a to stop mass shootings using the argument that the big gov wants to come and take away your guns  no ones saying that im not saying that that hasnt been said in this conversation  what people are saying however is that the original authors of the a could not have foreseen a country so armed and with weapons so powerful  the constitution is a living document and at times this requires rethinking certain aspects of it so that we as a country can better move forward  people want tighter regulations on owning guns so that people like adam lanza could not have ready access to them so that cruz could not have gone on a rampage that we saw coming miles away  we require a system that can foresee threats and stop them before they become another tragedy i e  the cdc researching gun violence and mental illness\"\n",
    "cleaned, tokens = clean_text(new_doc, lemmatize=True, stem=False)\n",
    "tokens = pland_clean(tokens)\n",
    "bow = dictionary.doc2bow(tokens)\n",
    "bow_dict = dict()\n",
    "for key, value in ldamodel.get_document_topics(bow):\n",
    "    bow_dict[key] = value\n",
    "sort_dict(bow_dict, by_value=True, reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17789"
      ]
     },
     "execution_count": 321,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ldamodel.num_terms "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(corpus, open('corpus.pkl', 'wb'))\n",
    "dictionary.save('dictionary.gensim')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
